#  keboola.ex-kafka 0.0.1

_Released on 2025-05-07_

**Component:** [keboola.ex-kafka](https://github.com/keboola/component-kafka)  
**Tag:** [0.0.1](https://github.com/keboola/component-kafka/releases/tag/0.0.1)  
**Stage:** PRIVATE


## Change log (AI generated):
## New! Kafka Consumer for Seamless Data Streaming
Ingest data directly from your Apache Kafka topics into Keboola with our brand new Kafka Consumer component.

We're excited to announce the launch of our Kafka Consumer, a powerful new component designed to streamline your data integration workflows.
This initial release provides the foundational capability to connect to your Kafka clusters, consume messages from specified topics, and process them into CSV files within Keboola Storage.
Key features include robust Kafka offset commit handling to ensure data processing reliability (at-least-once semantics) and configurable consumer polling timeouts to fine-tune data ingestion.
Start leveraging your Kafka data streams within Keboola today for richer analytics and data operations.

---
## Kafka Consumer Upgraded: Advanced Deserialization, Security & Control
Unlock more from your Kafka streams with enhanced SSL security, Avro & JSON support, multi-topic consumption, and granular data fetching in our Kafka Consumer.

Our Kafka Consumer has received significant upgrades, bringing more power and flexibility to your data ingestion pipelines.
**Enhanced Data Handling & Security:**
*   **Versatile Deserialization:** Process a wider range of data formats with added support for Avro (via Schema Registry or Fastavro for schema-less scenarios) and JSON messages. Native data type preservation has also been improved for higher data fidelity in your output.
*   **Secure Connections:** Connect to Kafka brokers securely using SSL/TLS encryption, ensuring your data is protected in transit.
**Greater Control & Output Refinement:**
*   **Multi-Topic Consumption:** Ingest data from multiple Kafka topics simultaneously using a single component configuration.
*   **Precise Data Fetching:** Gain fine-grained control with manual offset and partition overrides, allowing you to specify exactly which data range from your topics is processed.
*   **Improved CSV Output:** Avro fields can now be mapped directly to individual CSV columns, making complex data structures more accessible for analysis. The primary key for output tables has been changed to use the Kafka message key and offset, enhancing record uniqueness for incremental loads. Stateful column information management also helps maintain consistent output structure, especially with evolving Avro schemas.
**Usability & Performance:**
*   **Dynamic IDs:** `group_id` and `client_id` for the Kafka consumer can now be generated dynamically based on patterns, simplifying management and improving how consumer groups track offsets.
*   **Topic Discovery:** A new synchronous action allows you to easily list available Kafka topics directly from the component, aiding in configuration.
*   **File Naming:** Output CSV file naming conventions have been updated for better organization.
*   **Performance Boost:** Caching of CSV writer instances speeds up data output operations.
These enhancements empower you to tackle more complex Kafka integration scenarios with greater ease, security, and control over your data.

---
## New! Kafka Producer: Stream Data from Keboola to Kafka
Introducing the Kafka Producer component, enabling you to easily send data from your Keboola Storage tables directly to Apache Kafka topics.

We're thrilled to expand our Kafka integration capabilities with the launch of the new Kafka Producer component!
Now, you can effortlessly stream data *from* your Keboola project *to* your Kafka topics. This opens up new possibilities for real-time data distribution, event-driven architectures, and integrating Keboola-processed data with other downstream systems.
The Kafka Producer reads data from your input tables in Keboola Storage, serializes it, and efficiently sends it to the specified Kafka topic(s). It includes robust Kafka client logic for reliable message production and leverages Pydantic for clear and validated configurations.
Start pushing your transformed and enriched data from Keboola into your Kafka ecosystem today.



## Changes:



- init 




- logging fixes 




- Merged in fisa (pull request #1) 




- few experimentation 




- Merged in fisa (pull request #2) 




- Initial commit 




- codestyle, commit handling 




- fix csv storing, manual offset override support 




- Merged in self-offset-handling-consumer (pull request #3) 




- scripts executable 






- Merged in self-offset-handling-consumer (pull request #4) 






- Merged in self-offset-handling-consumer (pull request #5) 




- dos2unix le 




- Merged in self-offset-handling-consumer (pull request #6) 




- empty desc 




- add pass hash, fix partition override 




- Merged in update/offset-override-pss-hash (pull request #7) 




- Add logging 




- Add verb logging 






- Merge branch 'master' of bitbucket.org:kds_consulting_team/ex-kafka-consumer 




- FIX timeout 




- UPDATE logging level consumer 




- UPDAte logging level 




- UPDAte logging level add broker 




- UpDATE Readme 




- Update readme 




- updated component framework, moved config to pydantic configuration.py 




- added SSL support 




- update baseimage to python 3.12 




- flake8 




- deserialization support for avro, schema registry 




- dependencies 




- extracting multiple topics, sync action list_topics 




- added option to include Avro columns as individual columns in CSV 




- changed csv files naming 




- pipeline 




- tests: text message, avro as value 




- CLRF 




- native types support 




- UI 




- use pattern for group_id and client_id 




- fix config 




- sleep 




- Revert "sleep" 




- env vars 




- allowed host from stack parameters 




- freeze requirements and schema registry image version 




- older schema registry image 




- older kafka image 






- config, readme 




- review feedback - added sync action for message preview 




- fix test 




- PR #1 




- repo reorganization 




- added 2nd component 




- prepare client for writer 








- first writer 




- removed old tests 




- added tests 




- UI, remove unused parameters from configuration + test config.json 








- fix config init params 




- readme 




- review 




- review 1 - renamed config param from servers to bootstrap_servers 




- changed pk of extractor to key + offset 




- cached writers for improved performance 




- ruff, new datadirtest version 




- ruff 




- Fix typos in README.md 




- extractor - fix config params, update common requirements 




- writer - fix config params 




- added runtime logging, added json deserializer option 




- added Fastavro deserializer for deserialization without a schema registry 






- fix dockerfile, added schema registry mention to readme files, requirements fix 




- fix tests 




- new schema registry image 




- PR #2 




- added VERSION files to wr-kafka and ex-kafka 




- PR #4 




- fix ui 




- added new SASL_SSL option to the UI, added descriptions, move version file 




- clarify extractor description, add note that JSON is schemaless 




- PR #5 



[Compare on GitHub](https://github.com/keboola/component-kafka/compare/initial...0.0.1)



## Component Information
**Type:** extractor
**Name:** Kafka

**Description:** Apache Kafka is an open-source log-based event streaming platform.


**Documentation:** [Link](https://github.com/keboola/component-kafka/blob/master/components/ex-kafka/README.md)



---
_Generated by [Release Notes Generator](https://github.com/keboola/release-notes-generator)
on 2025-05-22 09:15:33_