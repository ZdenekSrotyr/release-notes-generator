#  keboola.wr-kafka 0.0.1

_Released on 2025-05-07_

**Component:** [keboola.wr-kafka](https://github.com/keboola/component-kafka)  
**Tag:** [0.0.1](https://github.com/keboola/component-kafka/releases/tag/0.0.1)  
**Stage:** PRIVATE


## Change log (AI generated):
## Unlock Your Kafka Data: Introducing the New Keboola Kafka Extractor
Easily stream data from Apache Kafka topics directly into Keboola Storage with our powerful new Kafka Extractor, featuring robust offset management, Avro/JSON deserialization, and flexible output formatting.

We're excited to announce the launch of the Keboola Kafka Extractor (keboola.ex-kafka), a new component designed to seamlessly integrate your Apache Kafka data streams with your Keboola projects.
The Kafka Extractor allows you to consume messages from one or more Kafka topics, process them, and write the data into Keboola Storage tables.

Key features include:
*   **Comprehensive Data Ingestion:** Consume messages and enrich them with Kafka metadata such as message key, partition, and offset, providing full context for your data. The output table primary key is set to `["key", "offset"]` for unique identification in Keboola Storage.
*   **Reliable Offset Management:** Ensures data is processed reliably with stateful offset handling. By default, it consumes from the earliest available message and automatically saves the *next* offset to be consumed, preventing data loss or reprocessing under normal conditions. You can also manually override starting offsets for precise data pulls.
*   **Advanced Deserialization:** Supports various message formats. Deserialize Avro messages using Confluent Schema Registry or a local schema file (FastAvro), and also handle plain JSON messages.
*   **Flexible Output Formatting:** Choose how your data lands in Keboola. Output raw message values or "flatten" deserialized Avro/JSON objects, where top-level fields become individual columns in your output table. Specific Avro types like `bytes`, `fixed` (including `decimal` logical types), and other byte arrays are handled intelligently during flattening (e.g., decimals as strings, other bytes as Base64).
*   **Multi-Topic Extraction:** Configure the extractor to pull data from multiple Kafka topics in a single run, each mapping to its own output table and maintaining independent offset tracking.
*   **Smart Defaults & Robust Connectivity:** Sensible defaults for consumer `group_id` and `client_id` are automatically generated based on your project, configuration, and topic context. The component uses standardized Kafka client connection logic for secure and reliable communication, supporting SSL and SASL.

Getting started is easy. Configure your Kafka connection, specify your topics, and let the extractor handle the complexities of data streaming. This new component empowers you to leverage your real-time Kafka data within the powerful Keboola ecosystem for analytics, transformation, and more.

## Seamlessly Send Keboola Data to Kafka with Our New Writer Component
Introducing the Keboola Kafka Writer (keboola.wr-kafka), enabling you to easily publish data from your Keboola Storage tables directly to Apache Kafka topics with flexible serialization options.

We're thrilled to expand our Kafka integration capabilities with the launch of the Keboola Kafka Writer (keboola.wr-kafka). This new component allows you to take data processed or managed within Keboola and send it as messages to your Apache Kafka topics.
The Kafka Writer reads data from specified input CSV tables in your Keboola project and produces messages to one or more Kafka topics.

Key features include:
*   **Flexible Data Sourcing:** Configure input mappings to select which Keboola Storage tables provide the data for your Kafka messages.
*   **Targeted Kafka Production:** Specify the Kafka topic(s) where your data should be sent.
*   **Versatile Serialization:**
    *   **String Serialization:** Send data as plain string messages.
    *   **Avro Serialization:** Encode messages in Avro format. You can leverage a Confluent Schema Registry for schema management or provide a local Avro schema file directly in your configuration.
*   **Robust Connection Handling:** Utilizes standardized Kafka client connection logic for secure and reliable communication with your Kafka brokers, supporting SSL and SASL authentication mechanisms.

The Kafka Writer empowers you to close the loop on your data workflows, enabling you to feed insights generated in Keboola back into your real-time Kafka streams for operational systems, further event processing, or sharing with other applications.



## Changes:



- init 




- logging fixes 




- Merged in fisa (pull request #1) 




- few experimentation 




- Merged in fisa (pull request #2) 




- Initial commit 




- codestyle, commit handling 




- fix csv storing, manual offset override support 




- Merged in self-offset-handling-consumer (pull request #3) 




- scripts executable 






- Merged in self-offset-handling-consumer (pull request #4) 






- Merged in self-offset-handling-consumer (pull request #5) 




- dos2unix le 




- Merged in self-offset-handling-consumer (pull request #6) 




- empty desc 




- add pass hash, fix partition override 




- Merged in update/offset-override-pss-hash (pull request #7) 




- Add logging 




- Add verb logging 






- Merge branch 'master' of bitbucket.org:kds_consulting_team/ex-kafka-consumer 




- FIX timeout 




- UPDATE logging level consumer 




- UPDAte logging level 




- UPDAte logging level add broker 




- UpDATE Readme 




- Update readme 




- updated component framework, moved config to pydantic configuration.py 




- added SSL support 




- update baseimage to python 3.12 




- flake8 




- deserialization support for avro, schema registry 




- dependencies 




- extracting multiple topics, sync action list_topics 




- added option to include Avro columns as individual columns in CSV 




- changed csv files naming 




- pipeline 




- tests: text message, avro as value 




- CLRF 




- native types support 




- UI 




- use pattern for group_id and client_id 




- fix config 




- sleep 




- Revert "sleep" 




- env vars 




- allowed host from stack parameters 




- freeze requirements and schema registry image version 




- older schema registry image 




- older kafka image 






- config, readme 




- review feedback - added sync action for message preview 




- fix test 




- PR #1 




- repo reorganization 




- added 2nd component 




- prepare client for writer 








- first writer 




- removed old tests 




- added tests 




- UI, remove unused parameters from configuration + test config.json 








- fix config init params 




- readme 




- review 




- review 1 - renamed config param from servers to bootstrap_servers 




- changed pk of extractor to key + offset 




- cached writers for improved performance 




- ruff, new datadirtest version 




- ruff 




- Fix typos in README.md 




- extractor - fix config params, update common requirements 




- writer - fix config params 




- added runtime logging, added json deserializer option 




- added Fastavro deserializer for deserialization without a schema registry 






- fix dockerfile, added schema registry mention to readme files, requirements fix 




- fix tests 




- new schema registry image 




- PR #2 




- added VERSION files to wr-kafka and ex-kafka 




- PR #4 




- fix ui 




- added new SASL_SSL option to the UI, added descriptions, move version file 




- clarify extractor description, add note that JSON is schemaless 




- PR #5 



[Compare on GitHub](https://github.com/keboola/component-kafka/compare/initial...0.0.1)



## Component Information
**Type:** writer
**Name:** Kafka

**Description:** Apache Kafka is an open-source log-based event streaming platform.


**Documentation:** [Link](https://github.com/keboola/component-kafka/blob/master/components/wr-kafka/README.md)



---
_Generated by [Release Notes Generator](https://github.com/keboola/release-notes-generator)
on 2025-05-22 09:20:28_