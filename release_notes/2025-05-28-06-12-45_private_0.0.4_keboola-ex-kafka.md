#  keboola.ex-kafka 0.0.4

_Released on 2025-05-28_

**Component:** [keboola.ex-kafka](https://github.com/keboola/component-kafka)  
**Tag:** [0.0.4](https://github.com/keboola/component-kafka/releases/tag/0.0.4)  
**Stage:** PRIVATE


## Change log (AI generated):
## Kafka Extractor: Default JSON Handling Updated
The Kafka Extractor now keeps nested JSON as a single field by default, offering more flexibility in how you process your Kafka messages.
Previously, the `flatten_message_value_columns` parameter defaulted to `True`, automatically breaking down nested JSON structures within your Kafka message values into separate columns. This default has now been changed to `False`.
This means, for new configurations or those not explicitly setting this parameter, nested JSON data will be delivered as a single field containing the complete JSON object or string. This change provides a more standard representation of JSON data and can simplify initial setup if you prefer to handle parsing downstream.
If you rely on the previous automatic flattening behavior, please ensure you explicitly set `flatten_message_value_columns` to `True` in your extractor configuration to maintain your existing data structures.

## Kafka Extractor: Get Clearer Insights with Improved Message Preview
We've enhanced the `message_preview` synchronous action in our Kafka Extractor, ensuring you see an accurate and reliable sample of your topic data during component configuration.
A fix has been implemented to address issues in how data was fetched, deserialized, and presented in the message preview. This ensures that the sample messages you see accurately reflect the content of your Kafka topic.
This improvement allows for a smoother and more confident configuration experience, as you can trust the preview to correctly display your message structures and content, helping you set up your data extraction correctly from the start.

## New! Customize Your Kafka Data Destination in Keboola
Gain more control with the Kafka Extractor: now you can set custom table names and define load types (incremental/full) for your data within Keboola Storage.
We've added two new configuration parameters to the Kafka Extractor:
- `table_name`: You can now specify the exact name for your destination table in Keboola Storage.
- `load_type`: Choose how data is written to your table, with options such as "incremental" for appending new data or "full" for complete table refreshes.
These additions provide greater flexibility in organizing your extracted Kafka data and managing how it integrates with your existing storage strategies, directly impacting how data is structured and stored in your Keboola project.

## Power Up Your Kafka Writes: Define Custom Message Keys
Our Kafka Writer component now lets you specify a message key for each message produced, improving data partitioning, ordering, and enabling advanced Kafka features.
You can now configure the writer to include a specific key with each message sent to your Kafka topic. The Kafka message key is crucial metadata used by Kafka brokers to determine the partition to which a message is sent.
By defining message keys, you can ensure that all messages with the same key are routed to the same partition. This is vital for maintaining message order for related events and is a prerequisite for features like Kafka's log compaction. This enhancement gives you finer control over your data streams and how they are processed by Kafka.

## Important Update: Kafka Extractor `group_id` Management
The `group_id` configuration for the Kafka Extractor is now managed internally, removing the UI option. Please monitor your extractions carefully following this change.
The `group_id` parameter for the Kafka consumer, previously configurable via the UI, will now be determined automatically by the component. The `group_id` is fundamental to how Kafka consumers manage offsets and participate in consumer groups.
This change means the effective `group_id` used by the extractor might differ from previously configured values. If the `group_id` changes, the extractor might connect as a new consumer group. This could potentially lead to re-processing data from the beginning of the topic or, in some scenarios, skipping messages if offsets for the new group are not as expected. We advise you to closely monitor your Kafka data extractions to ensure data is consumed as intended.

## Kafka Extractor: Enhanced Data Reliability with Manual Offset Commits
We've upgraded the Kafka Extractor to use manual offset committing, significantly improving data processing integrity and consistency by giving the component precise control over message acknowledgment.
The Kafka consumer's `enable_auto_commit` feature has been disabled. Instead, the extractor now manually commits offsets after messages are successfully processed and written to Keboola Storage.
This change is a key improvement for data processing reliability. Manual offset committing helps prevent data loss (e.g., if the component crashes after an auto-commit but before processing is complete) and reduces the chances of data duplication. This leads to stronger at-least-once processing guarantees, ensuring your data is handled more robustly.



## Changes:



- use textarea for ssl fields 




- EX: changed default value of flatten_message_value_columns to False 




- EX: fix message_preview sync action 




- EX: added load type and table name params, raise Kafka exceptions as UserException 




- WR: added option to define a key on the config row 




- EX: removed group_id from UI 




- EX: closing consumer, disable auto commit 




- PR #9 



[Compare on GitHub](https://github.com/keboola/component-kafka/compare/0.0.3...0.0.4)



## Component Information
**Type:** extractor
**Name:** Kafka

**Description:** Apache Kafka is an open-source log-based event streaming platform.


**Documentation:** [Link](https://github.com/keboola/component-kafka/blob/master/components/ex-kafka/README.md)



---
_Generated by [Release Notes Generator](https://github.com/keboola/release-notes-generator)
on 2025-05-29 13:01:20_