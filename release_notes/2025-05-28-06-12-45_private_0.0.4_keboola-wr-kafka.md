#  keboola.wr-kafka 0.0.4

_Released on 2025-05-28_

**Component:** [keboola.wr-kafka](https://github.com/keboola/component-kafka)  
**Tag:** [0.0.4](https://github.com/keboola/component-kafka/releases/tag/0.0.4)  
**Stage:** PRIVATE


## Change log (AI generated):
## Kafka Extractor: Default JSON Handling Updated
The Kafka Extractor's default for processing JSON message values has changed; nested data will now be preserved as a single field unless specified otherwise.

We've updated the default behavior for the `flatten_message_value_columns` parameter in our Kafka Extractor. Previously, this parameter was often defaulted to `True`, which meant nested JSON structures within the Kafka message `value` were automatically flattened into separate columns in your output tables.

Effective now, the default value for `flatten_message_value_columns` is `False`. If you do not explicitly set this parameter in your configuration, nested JSON within the message `value` will no longer be automatically flattened. Instead, the `value` will be output as a single field, typically containing the raw JSON string or a nested object structure. This change provides more direct access to the original JSON and offers greater flexibility for downstream transformations. Please review your Kafka Extractor configurations if you were relying on the previous default auto-flattening behavior, as your output table structure may be affected.

---
## Kafka Extractor: New Controls for Load Type & Table Naming
Gain more granular control over your Kafka data ingestion with new `load_type` and `table_name` parameters in the Kafka Extractor.

The Kafka Extractor now offers enhanced configuration options with the introduction of two new parameters: `load_type` and `table_name`. These parameters give you more precise control over your data ingestion workflows into Keboola Storage.

With `load_type`, you can specify how extracted data is loaded, choosing between options such as 'incremental' or 'full' loads, tailoring the data update strategy to your needs. The `table_name` parameter allows you to directly define the destination table name for your extracted data within Keboola Storage. These additions provide greater flexibility in managing the final state and organization of your data, streamlining your data pipeline management.

---
## Kafka Writer Enhanced: Custom Message Keys Now Available
Our Kafka Writer now supports custom message keys, offering improved data partitioning, ordering, and enabling advanced Kafka stream capabilities.

We're excited to announce a significant enhancement to our Kafka Writer: you can now define a Kafka message key for your outgoing messages. This option can be configured, for example, on a per-configuration row or per-table basis, giving you precise control.

Message keys are a fundamental aspect of Apache Kafka, used by the broker to determine the partition to which a message is sent. By setting a custom key, you ensure that messages with the same key are consistently routed to the same partition. This is crucial for maintaining message order for related events and is a prerequisite for features like Kafka's log compaction. This new capability provides you with finer-grained control over your data streams, enhancing data organization and processing within your Kafka topics.

---
## Kafka Extractor: Simplified & Safer Consumer Group IDs
Kafka Extractor now automatically manages consumer `group_id`s, simplifying configuration and enhancing the consistency of message processing.

We've updated how the Kafka consumer `group_id` is managed in the Kafka Extractor. This parameter, crucial for Kafka's consumer offset management and tracking which messages have been processed, is no longer configurable through the user interface.

Instead, the `group_id` will now be automatically determined by the component, often based on its unique configuration ID or a predefined scheme. This change simplifies the initial setup process and reduces the potential for misconfiguration. Automatic `group_id` management helps ensure better isolation between different extractor configurations consuming from the same topics and promotes consistent, reliable tracking of message consumption progress.

---
## Kafka Extractor: Enhanced Reliabilityâ€”Manual Offset Commits
Improve your Kafka data extraction integrity; the Extractor now uses manual offset committing to significantly reduce the risk of data loss.

To further enhance the robustness of your data pipelines, the Kafka consumer within our Extractor has been updated to disable automatic offset committing by setting `enable.auto.commit` to `False`. We have implemented manual offset committing logic within the component.

This change means that message consumption offsets are now committed to Kafka only after the data has been successfully processed and persisted into Keboola Storage. This moves the extractor towards at-least-once processing semantics, significantly reducing the risk of data loss if the component encounters an issue mid-process. This improvement, along with more explicit consumer resource management, ensures that your data extraction from Kafka is more reliable and resilient.



## Changes:



- use textarea for ssl fields 




- EX: changed default value of flatten_message_value_columns to False 




- EX: fix message_preview sync action 




- EX: added load type and table name params, raise Kafka exceptions as UserException 




- WR: added option to define a key on the config row 




- EX: removed group_id from UI 




- EX: closing consumer, disable auto commit 




- PR #9 



[Compare on GitHub](https://github.com/keboola/component-kafka/compare/0.0.3...0.0.4)



## Component Information
**Type:** writer
**Name:** Kafka

**Description:** Apache Kafka is an open-source log-based event streaming platform.


**Documentation:** [Link](https://github.com/keboola/component-kafka/blob/master/components/wr-kafka/README.md)



---
_Generated by [Release Notes Generator](https://github.com/keboola/release-notes-generator)
on 2025-05-29 13:01:57_