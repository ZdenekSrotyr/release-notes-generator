#  keboola.ex-kafka 0.0.4

_Released on 2025-05-29_

**Component:** [keboola.ex-kafka](https://github.com/keboola/component-kafka)  
**Tag:** [0.0.4](https://github.com/keboola/component-kafka/releases/tag/0.0.4)  
**Stage:** PRIVATE


## Change log (AI generated):
## Kafka Extractor Gets Major Upgrades: Enhanced Reliability, Control, and Data Handling
Our Kafka extractor has received significant updates, boosting data processing reliability, offering new loading controls, adjusting default JSON message handling, and improving error clarity.

The Keboola Kafka extractor has undergone several key enhancements designed to provide a more robust, flexible, and user-friendly experience for ingesting data from your Kafka topics. These changes focus on improving data integrity, offering finer-grained control over data loading, and refining how data is handled and errors are communicated.

### Boosted Reliability and Stability
Data processing reliability is paramount. The extractor now employs manual offset commits by default (`enable.auto.commit` is `False`). This is a critical improvement that prevents message loss if processing fails after a message is consumed but before it would have been auto-committed, and also avoids skipping messages. This leads to stronger "at-least-once" data delivery guarantees. Complementing this, the Kafka consumer is now explicitly closed after processing, ensuring better resource management and overall component stability.

### Greater Control Over Data Ingestion
You gain more precise control over how Kafka data lands in Keboola Storage. We've introduced new configuration parameters:
*   `load_type`: Specify whether data should append to or overwrite the destination table (e.g., for incremental or full loads).
*   `table_name`: Directly define the name of the destination table in Keboola Storage.

Additionally, please note that the `group_id` parameter for the Kafka consumer is no longer configurable via the user interface. It is now managed internally by the component. This change may affect consumer group behavior if you previously relied on specific, manually set `group_id`s for distinct consumer tracking.

### Refined Data Handling and User Experience
The default behavior for handling JSON messages has been updated. The `flatten_message_value_columns` parameter now defaults to `False`. This means JSON message values from Kafka topics will no longer be automatically flattened into separate columns. Instead, the raw or structured JSON value will typically be placed into a single column. If your workflows depend on the previous auto-flattening behavior, you will need to explicitly set this parameter to `True` in your extractor configuration.

To enhance troubleshooting, Kafka-related exceptions encountered by the extractor are now re-raised as `UserException`. This provides you with more informative and context-specific error messages when issues arise. Lastly, the `message_preview` synchronous action has been fixed, ensuring more accurate and reliable previews of your Kafka messages.

---
## Kafka Writer Enhanced: Define Custom Message Keys
The Kafka writer now empowers you with finer control over your data streams by allowing the definition of a message key for messages sent to Kafka.

Our Kafka writer has been updated to support an important Kafka feature: user-defined message keys. This enhancement allows you to specify a key for each message produced to your Kafka topics, directly influencing how these messages are partitioned, ordered, and managed within Kafka.

### Take Control with Custom Message Keys
You can now configure the Kafka writer to derive a message key from data within your configuration row. This key becomes a fundamental attribute of the messages sent to Kafka, unlocking several benefits:

*   **Improved Partitioning:** Kafka uses the message key to determine which partition a message is routed to. By setting custom keys, you can achieve more predictable data distribution and colocate related messages.
*   **Enhanced Ordering:** While Kafka guarantees message order within a single partition, using the same key for related messages ensures they are all sent to the same partition, thus preserving their relative order.
*   **Log Compaction Compatibility:** For Kafka topics configured with log compaction (where only the latest message for each key is retained), defining a message key is essential for this feature to work correctly.

This update provides you with a powerful mechanism to better leverage Kafka's capabilities for optimized data processing, stream management, and integration with downstream systems that rely on message keys.



## Changes:



- use textarea for ssl fields 




- EX: changed default value of flatten_message_value_columns to False 




- EX: fix message_preview sync action 




- EX: added load type and table name params, raise Kafka exceptions as UserException 




- WR: added option to define a key on the config row 




- EX: removed group_id from UI 




- EX: closing consumer, disable auto commit 




- PR #9 




- EX + WR: changed version files to 0.0.4 




- PR #10 



[Compare on GitHub](https://github.com/keboola/component-kafka/compare/0.0.3...0.0.4)



## Component Information
**Type:** extractor
**Name:** Kafka

**Description:** Apache Kafka is an open-source log-based event streaming platform.


**Documentation:** [Link](https://github.com/keboola/component-kafka/blob/master/components/ex-kafka/README.md)



---
_Generated by [Release Notes Generator](https://github.com/keboola/release-notes-generator)
on 2025-06-20 08:27:22_