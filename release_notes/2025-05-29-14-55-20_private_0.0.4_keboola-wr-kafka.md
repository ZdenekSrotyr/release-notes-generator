#  keboola.wr-kafka 0.0.4

_Released on 2025-05-29_

**Component:** [keboola.wr-kafka](https://github.com/keboola/component-kafka)  
**Tag:** [0.0.4](https://github.com/keboola/component-kafka/releases/tag/0.0.4)  
**Stage:** PRIVATE


## Change log (AI generated):
## Kafka Extractor: JSON Flattening Default Changed to Opt-In
The Kafka extractor's default behavior for handling JSON message values has been updated; nested structures will no longer be automatically flattened into separate columns unless explicitly configured.

We've changed the default setting for the `flatten_message_value_columns` parameter in the Kafka extractor from `True` to `False`. This means that if you haven't explicitly configured this parameter, the extractor will now output JSON message values as a single field containing the JSON string or nested structure. Previously, it would attempt to flatten these structures into multiple columns.

This change provides a more standard handling of JSON data by default. If your workflows rely on the previous automatic flattening behavior, please review your extractor configurations and explicitly set `flatten_message_value_columns` to `True` to maintain the existing output schema.

---
## New for Kafka Extractor: Customize Output Table Name & Load Type
Gain more control over your Kafka data ingestion with new options to define the output table name and load type directly in the extractor configuration.

We're pleased to introduce two new configuration parameters for the Kafka extractor: `table_name` and `load_type`. You can now specify the desired name for your output table in Keboola Storage (defaulting to "kafka_events") and choose the load type, such as "INCREMENTAL" (default) or "FULL".

These additions offer greater flexibility in organizing your extracted data within Keboola Connection and allow you to tailor the data loading strategy to your specific needs, enhancing how your Kafka data is stored and managed.

---
## Kafka Writer Enhanced: Set Message Keys Using Table Column Data
Our Kafka writer now allows you to specify an input table column to be used as the Kafka message key, improving message routing and identification within your Kafka topics.

We've added a new `message_key_column` configuration parameter to the Kafka writer. When this parameter is set with a column name from your input table, the writer will use the value from that column as the key for each message produced to Kafka.

Using message keys is crucial for effective Kafka usage. Keys influence message partitioning, ensuring related messages can be processed together, and are vital for message identification and potential ordering within partitions by downstream consumers. This enhancement gives you more precise control over your data flow into Kafka.

---
## Kafka Extractor: `group_id` Management Update & Message Preview Changes
The `group_id` for Kafka extractors is now managed globally via Kafka credentials, and the `message_preview` action reflects updated offset handling.

We've streamlined `group_id` configuration for the Kafka extractor by removing it from the row-level UI settings. The consumer `group_id` for data extraction will now be sourced from your global Kafka credentials. If not set there, no `group_id` will be used.

The `message_preview` action will now use the `auto_offset_reset` setting from your extractor's row configuration. However, as `group_id` is no longer set at the row level, the preview will always operate without a `group_id`. This means the preview may not reflect the exact behavior of a main extraction run if a global `group_id` is configured in your Kafka credentials, particularly regarding offset management and which messages are displayed.

---
## Action Required: Kafka Extractor Consumer Offset Handling Changed, Risk of Duplicates
Kafka consumer auto-commit is now disabled in the extractor. Without corresponding manual offset commits, this change will cause messages to be re-processed on each run, likely leading to significant data duplication in your output.

A critical change has been made to how the Kafka extractor handles consumer offsets: the `enable_auto_commit` property for the Kafka consumer is now set to `False` for the main data extraction process. Crucially, manual offset commit logic has not been implemented in the primary consumption loop.

This means that consumer offsets will not be automatically (or manually) committed to Kafka during an extraction run. Consequently, each time the component runs, the consumer will re-process messages from the last known committed offset (or from the position defined by `auto_offset_reset` if no offsets were ever committed for that group). This behavior will almost certainly lead to significant data duplication in your output tables unless this is specifically intended for "read-all" scenarios or if offset management is handled externally. Please review your Kafka extractor configurations and downstream processes immediately to mitigate potential data integrity issues. For the `message_preview` action, disabling auto-commit is appropriate as it prevents the preview from altering consumer offsets.



## Changes:



- use textarea for ssl fields 




- EX: changed default value of flatten_message_value_columns to False 




- EX: fix message_preview sync action 




- EX: added load type and table name params, raise Kafka exceptions as UserException 




- WR: added option to define a key on the config row 




- EX: removed group_id from UI 




- EX: closing consumer, disable auto commit 




- PR #9 




- EX + WR: changed version files to 0.0.4 




- PR #10 



[Compare on GitHub](https://github.com/keboola/component-kafka/compare/0.0.3...0.0.4)



## Component Information
**Type:** writer
**Name:** Kafka

**Description:** Apache Kafka is an open-source log-based event streaming platform.


**Documentation:** [Link](https://github.com/keboola/component-kafka/blob/master/components/wr-kafka/README.md)



---
_Generated by [Release Notes Generator](https://github.com/keboola/release-notes-generator)
on 2025-06-20 08:27:22_